{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKaNJen3OuuE"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFH_ADoMJR0Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W34PV0uhJTch"
      },
      "outputs": [],
      "source": [
        "#Project Data from Kaggle\n",
        "\n",
        "import kagglehub\n",
        "import os\n",
        "path = kagglehub.dataset_download(\"niyarrbarman/symptom2disease\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "df = pd.read_csv(os.path.join(path, 'Symptom2Disease.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6njwyiQlDIWo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faXFo7eOJanv"
      },
      "outputs": [],
      "source": [
        "#drop missing values\n",
        "df.dropna(subset = ['label','text'], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cdJ5VzNJa-5"
      },
      "outputs": [],
      "source": [
        "#Remove stopwords from preprocessed text. Lemmatization is applied to reduce words to their base or root form.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBlBrokXJcqd"
      },
      "outputs": [],
      "source": [
        "#Preprocess data\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Check if the input is a string, if not, return an empty string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    #Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['clean_text'] = df['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74ZptVDgJiGa"
      },
      "outputs": [],
      "source": [
        "#Check df\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND56I2BpJj_a"
      },
      "outputs": [],
      "source": [
        "# Medical Transcriptions Dataset\n",
        "path2 = kagglehub.dataset_download(\"pasindueranga/disease-prediction-based-on-symptoms\")\n",
        "print(\"Path to dataset files:\", path2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR09ntihKoBi"
      },
      "outputs": [],
      "source": [
        "df_mt = pd.read_csv(os.path.join(path2, 'dataset.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM71qA9pJsuH"
      },
      "outputs": [],
      "source": [
        "#Check medical transcription dataset\n",
        "df_mt.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojOlynwrJwKh"
      },
      "outputs": [],
      "source": [
        "#Clean keywords column (can change to another column data if needed)\n",
        "df_mt['clean_keywords'] = df_mt['symptoms'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTM5W-qQKqWc"
      },
      "outputs": [],
      "source": [
        "df_mt.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_unique = df['label'].value_counts()\n",
        "print('Number of Unique Disease:', df_unique)"
      ],
      "metadata": {
        "id": "5-oY00gt3hFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mt_unique = df_mt['disease'].value_counts()\n",
        "print('Number of Unique Disease:', df_mt_unique)"
      ],
      "metadata": {
        "id": "JeuOybhr3_uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wordcloud of all the diseases in the df dataset\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "text = ' '.join(df['label'])\n",
        "wordcloud= WordCloud (width = 500, height = 300, background_color = 'black').generate(text)\n",
        "plt.figure(figsize = (12,4))\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Diseases')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CGfPn4Bvt7Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wordcloud of sympotos for each particular disease in the df dataset\n",
        "for label in df['label'].unique():\n",
        "  text = ' '.join(df[df['label'] == label ]['clean_text'])\n",
        "  wordcloud= WordCloud (width = 400, height = 200, background_color = 'black').generate(text)\n",
        "\n",
        "  plt.figure(figsize = (10,4))\n",
        "  plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "  plt.axis('off')\n",
        "  plt.title(f'Word Cloud for Disease: {label}')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "1ezViFnrt7hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bigrams to understand the frequency of most common symptom patterns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vector= CountVectorizer(ngram_range = (2,2))\n",
        "X = vector.fit_transform(df['clean_text'])\n",
        "sum_of_words = X.sum(axis = 0)\n",
        "word_freqency = [(word, sum_of_words[0, idx]) for word, idx in vector.vocabulary_.items()]\n",
        "sort_words = sorted(word_freqency, key = lambda x: x[1], reverse = True)[:25]\n",
        "df_bigram = pd.DataFrame(sort_words, columns = ['Bigram', 'Frequency'])\n",
        "sns.barplot(x = 'Frequency', y = 'Bigram', data = df_bigram)\n",
        "plt.title('Bigram Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p2Y89ulEt7kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tF-r7R_N4t2u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATYheIRAKtUM"
      },
      "outputs": [],
      "source": [
        "#Setup text vectorization with custom variables\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,3), max_df=0.75,min_df=5, use_idf=True, smooth_idf=True,sublinear_tf=True, max_features=1000)\n",
        "tfIdfMat  = vectorizer.fit_transform(df['clean_text'].tolist() )\n",
        "feature_names = sorted(vectorizer.get_feature_names_out())\n",
        "print(feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5VyaoE6Oylk"
      },
      "outputs": [],
      "source": [
        "#Setup PCA model\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "tfIdfMat_reduced = pca.fit_transform(tfIdfMat.toarray())\n",
        "labels = df['label'].tolist()\n",
        "del feature_names[0:35]\n",
        "category_list = df.label.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16A0TByjOyn4"
      },
      "outputs": [],
      "source": [
        "#Data is then split into training and validation sets using the train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfIdfMat_reduced, labels, stratify=labels,random_state=1)\n",
        "print('Train_Set_Size:'+str(X_train.shape))\n",
        "print('Test_Set_Size:'+str(X_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhDxbp6VOysF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_id'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "label2id = {label: int(idx) for label, idx in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n",
        "id2label = {int(idx): label for label, idx in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc5Qc7lYOzxK"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_pandas(df[['text', 'label_id']].rename(columns={'label_id': 'label'}))\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoFl93cFh3kJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRYQ7aMch7YK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUnZ8nAOMeOI"
      },
      "source": [
        "# **Evaluations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU8FjShExrD3"
      },
      "outputs": [],
      "source": [
        "#checking column/row items\n",
        "df.sample(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWVW0Vh89QiA"
      },
      "outputs": [],
      "source": [
        "#checking column/row items\n",
        "df_mt.sample(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzEnOFL5_Jo2"
      },
      "outputs": [],
      "source": [
        "#Import Modules\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# KNN Model Training\n",
        "\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k) based on your dataset\n",
        "knn_classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CbxjG-O_UXi"
      },
      "outputs": [],
      "source": [
        "# Predictions\n",
        "\n",
        "predictions = knn_classifier.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYORtNRnNy7E"
      },
      "source": [
        "# **Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqpxRMyZ_aGO"
      },
      "outputs": [],
      "source": [
        "# Model Evaluation\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGjpsFQ5OCAK"
      },
      "source": [
        "# **Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUNLilpd_fnF"
      },
      "outputs": [],
      "source": [
        "#Import Modules\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "\n",
        "# Plotting confusion matrix as a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', xticklabels=df['label'].unique(), yticklabels=df['label'].unique())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu3ozjJlOF9h"
      },
      "source": [
        "## **Example #1**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Named Entity Recognition (NER) - spaCy, BERT, and Flair\n",
        "# --- Named Entity Recognition with 3 Models ---\n",
        "\n",
        "# 1. spaCy NER\n",
        "import spacy\n",
        "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_ner_spacy(text):\n",
        "    doc = nlp_spacy(text)\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "# 2. Hugging Face BERT NER\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "ner_pipeline_bert = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "def extract_ner_bert(text):\n",
        "    entities = ner_pipeline_bert(text)\n",
        "    return [(ent['word'], ent['entity_group']) for ent in entities]\n",
        "\n",
        "# 3. Flair NER\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger_flair = SequenceTagger.load(\"ner\")\n",
        "\n",
        "def extract_ner_flair(text):\n",
        "    sentence = Sentence(text)\n",
        "    tagger_flair.predict(sentence)\n",
        "    return [(ent.text, ent.tag) for ent in sentence.get_spans('ner')]\n",
        "\n",
        "# Apply all three models to the cleaned text\n",
        "df['named_entities_spacy'] = df['clean_text'].apply(extract_ner_spacy)\n",
        "df['named_entities_bert'] = df['clean_text'].apply(extract_ner_bert)\n",
        "df['named_entities_flair'] = df['clean_text'].apply(extract_ner_flair)\n",
        "\n",
        "# Show sample output\n",
        "df[['clean_text', 'named_entities_spacy', 'named_entities_bert', 'named_entities_flair']].head()\n",
        "\n"
      ],
      "metadata": {
        "id": "A48q3bj9PhLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snuoUKCSCNAR"
      },
      "outputs": [],
      "source": [
        "#Import Modules\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Example Usage\n",
        "example_symptom_1 = df_mt['clean_keywords'][16]\n",
        "\n",
        "# Preprocess the input symptom\n",
        "preprocessed_symptom = preprocess_text(example_symptom_1)\n",
        "\n",
        "# Transform the preprocessed symptom using the same vectorizer used during training\n",
        "#symptom_tfidf = tfidf_vectorizer.transform([preprocessed_symptom])\n",
        "\n",
        "# Predict the disease\n",
        "predicted_disease = knn_classifier.predict(tfIdfMat_reduced)\n",
        "actual_disease = df_mt['disease'][16]\n",
        "\n",
        "# Print the results\n",
        "print(f'Symptoms: {example_symptom_1}')\n",
        "print(f'Predicted Disease: {predicted_disease[0]}')\n",
        "print(f'Actual Disease: {actual_disease}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SGK0JoQOKRS"
      },
      "source": [
        "## **Example #2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGe-weasLIWU"
      },
      "outputs": [],
      "source": [
        "#Import Modules\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Example Usage\n",
        "example_symptom_1 = \"high fever\"\n",
        "\n",
        "# Preprocess the input symptom\n",
        "preprocessed_symptom = preprocess_text(example_symptom_1)\n",
        "\n",
        "# Transform the preprocessed symptom using the same vectorizer used during training\n",
        "#symptom_tfidf = tfidf_vectorizer.transform([preprocessed_symptom])\n",
        "\n",
        "# Predict the disease\n",
        "predicted_disease = knn_classifier.predict(tfIdfMat_reduced)\n",
        "#actual_disease = df_mt['disease'][16]\n",
        "\n",
        "# Print the results\n",
        "print(f'Symptoms: {example_symptom_1}')\n",
        "print(f'Predicted Disease: {predicted_disease[105]}')\n",
        "print(f'Actual Disease: {actual_disease}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example #3**"
      ],
      "metadata": {
        "id": "mWbVbAyRMKSq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XUt95PaxvyJ"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "\n",
        "def train_model(model, train_dataset, eval_dataset, tokenizer, output_dir=\"./results\"):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=5,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=output_dir + \"/logs\",\n",
        "        logging_steps=10,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    trainer.train()\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5maHJCm9x4W_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def evaluate_model(trainer, eval_dataset, label2id):\n",
        "    predictions = trainer.predict(eval_dataset)\n",
        "    preds = np.argmax(predictions.predictions, axis=1)\n",
        "    true_labels = predictions.label_ids\n",
        "    target_names = list(label2id.keys())\n",
        "\n",
        "    print(\"Accuracy:\", accuracy_score(true_labels, preds))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(true_labels, preds))\n",
        "    print(\"Classification Report:\\n\", classification_report(true_labels, preds, target_names=target_names))\n",
        "\n",
        "    # Confusion Matrix Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(confusion_matrix(true_labels, preds), annot=True, fmt=\"d\",\n",
        "                xticklabels=target_names, yticklabels=target_names, cmap=\"Blues\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Bar Plot of Precision, Recall, F1\n",
        "    report_dict = classification_report(true_labels, preds, target_names=target_names, output_dict=True)\n",
        "    metrics = [\"precision\", \"recall\", \"f1-score\"]\n",
        "    for metric in metrics:\n",
        "        values = [report_dict[label][metric] for label in target_names]\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(target_names, values)\n",
        "        plt.title(f\"{metric.title()} per Class\")\n",
        "        plt.ylim(0, 1)\n",
        "        plt.ylabel(metric.title())\n",
        "        plt.xlabel(\"Class\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOpUNMvkx_yX"
      },
      "outputs": [],
      "source": [
        "trainer = train_model(model, tokenized_dataset['train'], tokenized_dataset['test'], tokenizer)\n",
        "evaluate_model(trainer, tokenized_dataset['test'], label2id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def predict_disease_biobert(text, id2label):\n",
        "    result = classifier(text)[0]\n",
        "    disease_name = result['label']\n",
        "    score = result['score']\n",
        "    return disease_name, score"
      ],
      "metadata": {
        "id": "Nn9ktx2vPlMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import module (already done earlier)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Example symptom input\n",
        "symptom = df_mt['clean_keywords'][88]\n",
        "\n",
        "# Preprocess the symptom\n",
        "preprocessed_symptom = preprocess_text(symptom)\n",
        "\n",
        "# Predict the disease using the BioBERT pipeline\n",
        "predicted_disease, confidence = predict_disease_biobert(preprocessed_symptom, id2label)\n",
        "\n",
        "# Print results\n",
        "print(f'Symptoms: {symptom}')\n",
        "print(f'Predicted Disease: {predicted_disease} (Confidence: {confidence:.2f})')"
      ],
      "metadata": {
        "id": "FJ6EvqwvoXL6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}