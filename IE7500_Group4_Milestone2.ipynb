{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Depending on which objective we want to focus on, here are some framework options we can consider\n",
        "1. Symptom summarization:\n",
        "2. Medical Named Entity Recognition (NER): BioBERT/NER, model options en_core_sci_md\n",
        "\n",
        "    import spacy\n",
        "\n",
        "    import scispacy\n",
        "\n",
        "    nlp = spacy.load('en_core_sci_md')\n",
        "\n",
        "3. Symptom to Diagnosis Classification: knn/regression/random forests via sklearn"
      ],
      "metadata": {
        "id": "tNeKZ-jbMvLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "pKaNJen3OuuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "JFH_ADoMJR0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Project Data from Kaggle\n",
        "\n",
        "import kagglehub\n",
        "import os\n",
        "path = kagglehub.dataset_download(\"niyarrbarman/symptom2disease\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "df = pd.read_csv(os.path.join(path, 'Symptom2Disease.csv'))"
      ],
      "metadata": {
        "id": "W34PV0uhJTch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop missing values\n",
        "df.dropna(subset = ['label','text'], inplace = True)"
      ],
      "metadata": {
        "id": "faXFo7eOJanv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove stopwords from preprocessed text. Lemmatization is applied to reduce words to their base or root form.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "5cdJ5VzNJa-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess data\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Check if the input is a string, if not, return an empty string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    #Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['clean_text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "SBlBrokXJcqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check df\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "74ZptVDgJiGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Medical Transcriptions Dataset\n",
        "path2 = kagglehub.dataset_download(\"pasindueranga/disease-prediction-based-on-symptoms\")\n",
        "print(\"Path to dataset files:\", path2)\n"
      ],
      "metadata": {
        "id": "ND56I2BpJj_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mt = pd.read_csv(os.path.join(path2, 'dataset.csv'))"
      ],
      "metadata": {
        "id": "TR09ntihKoBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check medical transcription dataset\n",
        "df_mt.head(5)"
      ],
      "metadata": {
        "id": "FM71qA9pJsuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean keywords column (can change to another column data if needed)\n",
        "df_mt['clean_keywords'] = df_mt['symptoms'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "ojOlynwrJwKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mt.head(5)"
      ],
      "metadata": {
        "id": "tTM5W-qQKqWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup text vectorization with custom variables\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,3), max_df=0.75,min_df=5, use_idf=True, smooth_idf=True,sublinear_tf=True, max_features=1000)\n",
        "tfIdfMat  = vectorizer.fit_transform(df['clean_text'].tolist() )\n",
        "feature_names = sorted(vectorizer.get_feature_names_out())\n",
        "print(feature_names)"
      ],
      "metadata": {
        "id": "ATYheIRAKtUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup PCA model\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "tfIdfMat_reduced = pca.fit_transform(tfIdfMat.toarray())\n",
        "labels = df['label'].tolist()\n",
        "del feature_names[0:35]\n",
        "category_list = df.label.unique()"
      ],
      "metadata": {
        "id": "z5VyaoE6Oylk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data is then split into training and validation sets using the train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfIdfMat_reduced, labels, stratify=labels,random_state=1)\n",
        "print('Train_Set_Size:'+str(X_train.shape))\n",
        "print('Test_Set_Size:'+str(X_test.shape))"
      ],
      "metadata": {
        "id": "16A0TByjOyn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_id'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "label2id = {label: int(idx) for label, idx in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n",
        "id2label = {int(idx): label for label, idx in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n"
      ],
      "metadata": {
        "id": "HhDxbp6VOysF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_pandas(df[['text', 'label_id']].rename(columns={'label_id': 'label'}))\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n"
      ],
      "metadata": {
        "id": "yc5Qc7lYOzxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n"
      ],
      "metadata": {
        "id": "yoFl93cFh3kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "eRYQ7aMch7YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluations**"
      ],
      "metadata": {
        "id": "MUnZ8nAOMeOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking column/row items\n",
        "df.sample(1)"
      ],
      "metadata": {
        "id": "AU8FjShExrD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking column/row items\n",
        "df_mt.sample(1)"
      ],
      "metadata": {
        "id": "QWVW0Vh89QiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Modules\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# KNN Model Training\n",
        "\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k) based on your dataset\n",
        "knn_classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "GzEnOFL5_Jo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "\n",
        "predictions = knn_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "9CbxjG-O_UXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "oYORtNRnNy7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "id": "jqpxRMyZ_aGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Confusion Matrix**"
      ],
      "metadata": {
        "id": "MGjpsFQ5OCAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Modules\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "\n",
        "# Plotting confusion matrix as a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', xticklabels=df['label'].unique(), yticklabels=df['label'].unique())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AUNLilpd_fnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example #1**"
      ],
      "metadata": {
        "id": "hu3ozjJlOF9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Modules\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Example Usage\n",
        "example_symptom_1 = df_mt['clean_keywords'][16]\n",
        "\n",
        "# Preprocess the input symptom\n",
        "preprocessed_symptom = preprocess_text(example_symptom_1)\n",
        "\n",
        "# Transform the preprocessed symptom using the same vectorizer used during training\n",
        "#symptom_tfidf = tfidf_vectorizer.transform([preprocessed_symptom])\n",
        "\n",
        "# Predict the disease\n",
        "predicted_disease = knn_classifier.predict(tfIdfMat_reduced)\n",
        "actual_disease = df_mt['disease'][16]\n",
        "\n",
        "# Print the results\n",
        "print(f'Symptoms: {example_symptom_1}')\n",
        "print(f'Predicted Disease: {predicted_disease[0]}')\n",
        "print(f'Actual Disease: {actual_disease}')"
      ],
      "metadata": {
        "id": "snuoUKCSCNAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example #2**"
      ],
      "metadata": {
        "id": "5SGK0JoQOKRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Modules\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Example Usage\n",
        "example_symptom_1 = \"high fever\"\n",
        "\n",
        "# Preprocess the input symptom\n",
        "preprocessed_symptom = preprocess_text(example_symptom_1)\n",
        "\n",
        "# Transform the preprocessed symptom using the same vectorizer used during training\n",
        "#symptom_tfidf = tfidf_vectorizer.transform([preprocessed_symptom])\n",
        "\n",
        "# Predict the disease\n",
        "predicted_disease = knn_classifier.predict(tfIdfMat_reduced)\n",
        "#actual_disease = df_mt['disease'][16]\n",
        "\n",
        "# Print the results\n",
        "print(f'Symptoms: {example_symptom_1}')\n",
        "print(f'Predicted Disease: {predicted_disease[105]}')\n",
        "#print(f'Actual Disease: {actual_disease}')"
      ],
      "metadata": {
        "id": "RGe-weasLIWU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}